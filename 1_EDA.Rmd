---
title: "EDA,recipe and modeltime"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: false
      smooth_scroll: false
      number_sections: true
      fig_caption: TRUE
    css: "style.css"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
source(here("source","get_lib.R"))
```

<style>
.table-hover > tbody > tr:hover { 
  background-color: #f4f442;
}
</style>

# Raw data

```{r}
#read in data
mailchimp_users_tbl <- read_rds(here("data","mailchimp_users.rds"))
mailchimp_users_tbl %>% glimpse()

#counting number of events per day
#daily summary 
optins_day_tbl <- mailchimp_users_tbl %>% 
    summarise_by_time(
        .date_var = optin_time,
        .by = "day",
         optins = n()) 

optins_day_tbl%>% head()

#weekly summary 
mailchimp_users_tbl %>% 
    summarise_by_time(
        .date_var = optin_time,
        .by = "week",
        optins = n()) %>% head()

#monthly summary 
mailchimp_users_tbl %>% 
    summarise_by_time(
        .date_var = optin_time,
        .by = "month",
        optins = n()) %>% head()

```

## tk_summary_diagnostics()

-   `index`, the date or date-time column is called index

-   `units`: the description of a single timestamp within a time series

-   `scale`: the most common difference between timestamps within a time series. It is also called `interval`, `frequency`, `period` or `periodicity`

-   Differencey Summary

    -   Charaterizes the `scale` (interval) between time stamps in seconds
    -   1 day = 86,400 seconds

-   `optins_day_tbl` is irregular time sereis. Need to fill in gaps (i.e., missing days) before conducting the analysis

```{r}
#notice diff.mean 
optins_day_tbl %>% tk_summary_diagnostics(.date_var = optin_time)
```

## pad_by_time()

-   Performs time-series padding filling in any gaps, to convert the time series to a `regular` time series.

# Cleaning Data

## optins_day_prepared_tbl

```{r}
optins_day_tbl %>% 
    pad_by_time(
        .date_var = optin_time
    ) %>% head()

subscribers_daily_tbl <- mailchimp_users_tbl %>% 
    summarise_by_time(
        .date_var = optin_time,
        .by       = "day",
        optins    = n()
        ) %>% 
    pad_by_time(.by="day", .pad_value = 0)

subscribers_daily_tbl %>% head()

#notice diff.mean 
subscribers_daily_tbl %>% tk_summary_diagnostics(.date_var = optin_time)
```

## plot_time_series()

```{r}
subscribers_daily_tbl %>% 
    plot_time_series(.date_var = optin_time, .value = optins)
```

```{r}
subscribers_daily_tbl %>% 
 plot_anomaly_diagnostics(
     .date_var = optin_time,
     .value = optins,
     .alpha = 0.01
 )
```

- Perform log transformation

```{r}
subscribers_daily_tbl %>% 
    plot_time_series(optin_time, log(optins +1)) 
```



### ACF and PACF

- Transformation (i.e., `log1p`) is absolutely critical in identifying lags and using lags in models.
    - Without transformation the ACF plot shows almost no correlation.

- With transformation, we can see which lags are potential features. 
    - The plot below shows local maximum repeating after lag multiple of 7

```{r}
subscribers_daily_tbl %>% 
    plot_acf_diagnostics(optin_time, 
                         optins,
                         .lags = 100)
```

```{r}
subscribers_daily_tbl %>% 
    plot_acf_diagnostics(optin_time, log(optins+1))
```

```{r}
subscribers_daily_tbl %>% tk_stl_diagnostics(
        .date_var = optin_time,
        .value = optins
    )
```

- Why do we need to transform the data?
    - Without transformation, you could be predicting negative values.

```{r}
subscribers_daily_tbl %>%
    plot_time_series_regression(
        .date_var = optin_time,
        .formula = optins ~ as.numeric(optin_time) +
            wday(optin_time, label = TRUE) +
            month(optin_time, label = TRUE),
        .show_summary = TRUE
    )
```

## log transformatoin

```{r}
## The following code will return error
# subscribers_daily_tbl %>%
#     plot_time_series(optin_time, log(optins))

# Log Plus 1
subscribers_daily_tbl %>%
    plot_time_series(optin_time, log1p(optins))

# Inversion
subscribers_daily_tbl %>%
    plot_time_series(optin_time, log1p(optins) %>% expm1())

# Benefit
subscribers_daily_tbl %>%
    plot_time_series_regression(
        .date_var = optin_time,
        .formula = log1p(optins) ~ as.numeric(optin_time) +
            wday(optin_time, label = TRUE) +
            month(optin_time, label = TRUE),
        .show_summary = TRUE
    )
```

# Feature Engineering

## transformation 

- need to capture the parameters used for transformation

```{r}
data_prepared_tbl <- subscribers_daily_tbl %>%
     # Preprocessing
    mutate(optins_trans = log_interval_vec(optins, limit_lower = 0, offset = 1)) %>%
    mutate(optins_trans = standardize_vec(optins_trans)) %>%
    
    # Fix missing values at beginning of series
    filter_by_time(.start_date = "2018-07-03") %>%
    
    # Cleaning
    # replacing the outlier with the cleaned, the red plot during 
    # the time period specified within between_time()
    mutate(optins_trans_cleaned = ts_clean_vec(optins_trans, period = 7)) %>%
    mutate(optins_trans = ifelse(optin_time %>% between_time("2018-11-18", "2018-11-20"), 
                                 optins_trans_cleaned,
                                 optins_trans)) %>%
    
    select(-optins, -optins_trans_cleaned)



data_prepared_tbl %>%
    pivot_longer(contains("trans")) %>%
    plot_time_series(optin_time, value, name)
```


```{r}
#################################
# Save Key Params
# We need them to convert them back to
# original scale
##################################
limit_lower <- 0
limit_upper <- 3650.8
offset      <- 1
std_mean    <- -5.25529020756467
std_sd      <- 1.1109817111334
```

## data_prepared_full_tbl

### h vs new_data

1. Extend to Future Window
  - It is important to know your forecast horizon upfront.
  - This affects your ability to make features & how far to extend your `full dataset`

2. Add any lags to full dataset

```{r}
#prdiction horizon
horizon    <- 8*7

#M5 Competition, feature engineering was
#critical to success and this is something
#that M5 Competition winner did
#used to create rolling averages
#engineered features
#8 weeks and 7 days per week
lag_period <- 8*7
rolling_periods <- c(30, 60, 90)
```



### Create Xreg

- We can create additional features similar to `trend()` in TSLM.

```{r}
data_prepared_full_tbl <- data_prepared_tbl %>%
    
    # Add future window
    bind_rows(
        future_frame(.data = ., .date_var = optin_time, .length_out = horizon)
    ) %>%
    
    # Add Autocorrelated Lags
    tk_augment_lags(optins_trans, .lags = lag_period) %>% 
    
    # Add rolling features
    tk_augment_slidify(
        .value   = optins_trans_lag56,
        .f       = mean, 
        .period  = rolling_periods,
        .align   = "center",
        .partial = TRUE
    ) 

data_prepared_full_tbl %>% pivot_longer(-optin_time) %>% 
    plot_time_series(.date_var = optin_time,
                     value, name, .smooth= FALSE)

data_prepared_full_tbl %>% head() %>% 
     kable("html") %>% 
     kable_styling(bootstrap_options = c("striped", "hover"))

data_prepared_full_tbl %>% tail() %>% 
     kable("html") %>% 
    kable_styling(bootstrap_options = c("striped", "hover"))
```

## data_prepared_tbl and forecast_tbl

- SEPARATE INTO MODELING & FORECAST DATA 

```{r}
data_prepared_full_tbl %>% tail(57)

#will be used to create train and test
data_prepared_tbl <- data_prepared_full_tbl %>%
    filter(!is.na(optins_trans))
data_prepared_tbl

#will be used to make forecast
forecast_tbl <- data_prepared_full_tbl %>%
    filter(is.na(optins_trans))
forecast_tbl
```


## train/test

```{r}
#make the assess equal to your forecasting period
#Cumulative = TRUE uses all of the previous data in the dataset
splits <- time_series_split(data_prepared_tbl, assess = horizon, cumulative = TRUE)

splits %>%
    tk_time_series_cv_plan() %>%
    plot_time_series_cv_plan(optin_time, optins_trans)
```

## create base recipe

- It is important to try different feature engineering sets.
- `recipe()` defines the data preprocessing operations.

    - `recipes` are data dependent. New data must have the same column names and classes for the recipe to be applied.
    - Recipes are resuable, meaning we can use them for multiple models.
    - Recipes can be modified. We can add more steps and remove features. 
    
- Feature engineering is the most critical part of time series analysis.
    - Maximizing performance requires a lot of experimentation.  
    
- Multiple recipes
    - Can make a base recipe with most steps
    - Then create `model-specific` recipes that modify the base.
        - `spline model` in the example below will use natural splines to model trend.
        - `lag model` in the example below will use `Lag + Rolling` features to model trend. 
        
- Many ML model will return error if you fed it with time format data.

### step_timeseries_signature()

- adds a `preprocessing` step to generate the time series signature features. 

### step_rm()

- used to remove features that are unnecessary.
- `matches()` is a tidyselect helper that allows us to use Regular Expressions (RegEx) to select column names
    - used `()` to create multi-regex search patterns
    
### step_normalize()

- in `recipe` is equivalent to `standardize_vec()`
- recall that `timeTK` author calls normalization process as standardization

### step_range()
- is equivalent to `normalize_vec()`

### step_dummy()

- performs categorical encoding for either dummy encoding or one-hot encoding.

- Not all ML modles handles categorical data in this way. So need to perform preprocessing to be on the safe side. 

- `all_nominal()` a recipe column selector that selects any columns that are categorical.
    - See also: `all_numeric()` and `all_predictors()`

### step_fourier()

- Adds fourier series features

```{r}
recipe_spec_base <- recipe(optins_trans ~ ., data = training(splits)) %>%
    
    # Time Series Signature
    step_timeseries_signature(optin_time) %>%
    step_rm(matches("(iso)|(xts)|(hour)|(minute)|(second)|(am.pm)")) %>%
    
    # Standardization
    step_normalize(matches("(index.num)|(year)|(yday)")) %>%
    
    # Dummy Encoding (One Hot Encoding)
    step_dummy(all_nominal(), one_hot = TRUE) %>%
    
    # Interaction
    step_interact(~ matches("week2") * matches("wday.lbl")) %>%
    
    # Fourier
    step_fourier(optin_time, period = c(7, 14, 30, 90, 365), K = 2)

# juice() takes the training dataset out
recipe_spec_base %>% prep() %>% juice() %>% glimpse()
```

## recipe_spec_1

```{r}
#taking optin_time which is date time format out
#it still has index.num information in it. 
#it is also taking lag related features out
recipe_spec_1 <- recipe_spec_base %>%
    step_rm(optin_time) %>%
    #adds a step for a natural spline transformation 
    step_ns(ends_with("index.num"), deg_free = 2) %>%
    step_rm(starts_with("lag_"))
```


## recipe_spec_2

-  Lag Recipe 

### step_naomit()
- Removes rows with missing values from the columns specified. 

```{r}
recipe_spec_base %>% prep() %>% juice() %>% glimpse()

recipe_spec_2 <- recipe_spec_base %>%
    step_rm(optin_time) %>%
    #removes rows with missing value based on the values stored
    #under the column names that starts with lag_
    step_naomit(starts_with("optins_trans_lag56_"))
    

recipe_spec_2 %>% prep() %>% juice() %>% glimpse()
```

## recipe_spec_fourier

```{r}
recipe_spec_fourier <- recipe(optins_trans ~ optin_time, data = training(splits)) %>%
    step_fourier(optin_time, period = c(7, 14, 30, 90), K = 1) 
```


# Modeltime

## create model

- Don't fit the model at this point. 

```{r}
model_spec_lm <- linear_reg() %>%
    set_engine("lm")
```

## create workflow

- workflow requires `model` and `recipe` objects
- you need to fit the model within workflow before storing it into modeltime table

```{r}
workflow_fit_lm_1_spline <- workflow() %>%
    add_model(model_spec_lm) %>%
    add_recipe(recipe_spec_1) %>%
    fit(training(splits))
```

## model_tbl

- create modeltime table

```{r}
model_tbl <- modeltime_table(
    workflow_fit_lm_1_spline
)
```

## calibration_tbl

- create calibration table
- contains confidence interval

```{r}
calibration_tbl <- model_tbl %>%
    modeltime_calibrate(new_data = testing(splits))

calibration_tbl %>% modeltime_accuracy()
```

```{r}
calibration_tbl %>%
    modeltime_forecast(new_data    = testing(splits), 
                       actual_data = data_prepared_tbl) %>%
    plot_modeltime_forecast()
```

- Following the same process, let's create another workfolw object and store it inside modeltime

```{r}
workflow_fit_lm_2_lag <- workflow() %>%
    add_model(model_spec_lm) %>%
    add_recipe(recipe_spec_2) %>%
    fit(training(splits))

modeltime_tbl <- modeltime_table(
    workflow_fit_lm_1_spline,
    workflow_fit_lm_2_lag
) 
```


```{r}
#create prediction interval for two models
calibration_tbl <- modeltime_tbl %>%
    modeltime_calibrate(new_data = testing(splits))

#check the model performance
calibration_tbl %>% modeltime_accuracy()

#plot them
calibration_tbl %>%
    modeltime_forecast(new_data    = testing(splits), 
                       actual_data = data_prepared_tbl) %>%
    plot_modeltime_forecast()
```

## refit 

- Retrain each of your models on the full dataset.
    - This tends to improve performance. 

```{r}
refit_tbl <- calibration_tbl %>%
    modeltime_refit(data = data_prepared_tbl)
```

### Order of inversion operations

- The order of inversion is important.
    - It's always the reverse of the original transformation 

```{r}
refit_tbl %>%
    modeltime_forecast(new_data    = forecast_tbl,
                       actual_data = data_prepared_tbl) %>%
    
    # Invert Transformation
    mutate(across(.value:.conf_hi, .fns = ~ standardize_inv_vec(
        x    = .,
        mean = std_mean,
        sd   = std_sd
    ))) %>%
    mutate(across(.value:.conf_hi, .fns = ~ log_interval_inv_vec(
        x           = ., 
        limit_lower = limit_lower, 
        limit_upper = limit_upper, 
        offset      = offset
    ))) %>%
    
    plot_modeltime_forecast()
```


# Save the artifacts

```{r}
feature_engineering_artifacts_list <- list(
    # Data
    data = list(
        data_prepared_tbl = data_prepared_tbl,
        forecast_tbl      = forecast_tbl 
    ),
    
    # Recipes
    recipes = list(
        recipe_spec_base = recipe_spec_base,
        recipe_spec_1    = recipe_spec_1, 
        recipe_spec_2    = recipe_spec_2,
        recipe_spec_fourier = recipe_spec_fourier
    ),
    
    # Models / Workflows
    models = list(
        workflow_fit_lm_1_spline = workflow_fit_lm_1_spline,
        workflow_fit_lm_2_lag    = workflow_fit_lm_2_lag
    ),
    
    
    # Inversion Parameters
    standardize = list(
        std_mean = std_mean,
        std_sd   = std_sd
    ),
    log_interval = list(
        limit_lower = limit_lower, 
        limit_upper = limit_upper,
        offset      = offset
    )
    
)

feature_engineering_artifacts_list


feature_engineering_artifacts_list %>%
    write_rds("model/feature_engineering_artifacts_list.rds")


```

